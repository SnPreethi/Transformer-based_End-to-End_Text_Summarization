python main.py
[2025-09-04 11:19:04,428: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-09-04 11:19:04,443: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-09-04 11:19:04,450: INFO: common: yaml file: params.yaml loaded successfully]
[2025-09-04 11:19:04,450: INFO: common: Created directory at: artifacts]
[2025-09-04 11:19:04,450: INFO: common: Created directory at: artifacts/data_ingestion]
[2025-09-04 11:19:06,553: INFO: data_ingestion: artifacts/data_ingestion/data.zip download! with the following info:
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 5598:1EBEDF:4C043:F8B39:68B92851
Accept-Ranges: bytes
Date: Thu, 04 Sep 2025 05:49:06 GMT
Via: 1.1 varnish
X-Served-By: cache-hyd1100034-HYD
X-Cache: MISS
X-Cache-Hits: 0
X-Timer: S1756964946.755983,VS0,VE629
Vary: Authorization,Accept-Encoding
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: ede0584dac87400c01338d8256b8937ddd301369
Expires: Thu, 04 Sep 2025 05:54:06 GMT
Source-Age: 0

]
[2025-09-04 11:19:06,654: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-09-04 11:19:06,654: INFO: main: >>>>>> stage Data Validation stage started <<<<<<]
[2025-09-04 11:19:06,654: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-09-04 11:19:06,654: INFO: common: yaml file: params.yaml loaded successfully]
[2025-09-04 11:19:06,654: INFO: common: Created directory at: artifacts]
[2025-09-04 11:19:06,654: INFO: common: Created directory at: artifacts/data_validation]
[2025-09-04 11:19:06,654: INFO: main: >>>>>> stage Data Validation stage completed <<<<<<

x==========x]
[2025-09-04 11:19:06,654: INFO: main: >>>>>> stage Data Transformation stage started <<<<<<]
[2025-09-04 11:19:06,669: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-09-04 11:19:06,669: INFO: common: yaml file: params.yaml loaded successfully]
[2025-09-04 11:19:06,669: INFO: common: Created directory at: artifacts]
[2025-09-04 11:19:06,669: INFO: common: Created directory at: artifacts/data_transformation]
Map:   0%|                                                                            | 0/14732 [00:00<?, ? examples/s]C:\Users\preethisomayajula\anaconda3\envs\textS\lib\site-packages\transformers\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|██████████████████████████████████████████████████████████████| 14732/14732 [00:01<00:00, 7914.76 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████| 819/819 [00:00<00:00, 6892.55 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████| 818/818 [00:00<00:00, 7400.71 examples/s]
Saving the dataset (1/1 shards): 100%|████████████████████████████████| 14732/14732 [00:00<00:00, 521240.77 examples/s]
Saving the dataset (1/1 shards): 100%|████████████████████████████████████| 819/819 [00:00<00:00, 101703.43 examples/s]
Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████| 818/818 [00:00<?, ? examples/s]
[2025-09-04 11:19:11,781: INFO: main: >>>>>> stage Data Transformation stage completed <<<<<<

x==========x]
[2025-09-04 11:19:11,781: INFO: main: >>>>>> stage Model Training stage started <<<<<<]
[2025-09-04 11:19:11,789: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-09-04 11:19:11,791: INFO: common: yaml file: params.yaml loaded successfully]
[2025-09-04 11:19:11,791: INFO: common: Created directory at: artifacts]
[2025-09-04 11:19:11,791: INFO: common: Created directory at: artifacts/model_trainer]
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\preethisomayajula\Desktop\Transformer-based_End-to-End_Text_Summarization-master\src\textSummarizer\components\model_trainer.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(model=model_pegasus, args=training_args,
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.
  0%|                                                                                          | 0/921 [00:00<?, ?it/s]C:\Users\preethisomayajula\anaconda3\envs\textS\lib\site-packages\torch\utils\data\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
  1%|█                                                                                                                    1%|▌                                                                              | 6/921 [03:59<10:33:52, 41.57s/it] {'loss': 3.0679, 'grad_norm': 13.07686996459961, 'learning_rate': 9e-07, 'epoch': 0.01}
{'loss': 2.9845, 'grad_norm': 10.07537841796875, 'learning_rate': 1.9e-06, 'epoch': 0.02}
{'loss': 3.0904, 'grad_norm': 15.614034652709961, 'learning_rate': 2.9e-06, 'epoch': 0.03}
{'loss': 3.0822, 'grad_norm': 27.495195388793945, 'learning_rate': 3.9e-06, 'epoch': 0.04}
{'loss': 2.768, 'grad_norm': 15.819252967834473, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.05}
{'loss': 2.8917, 'grad_norm': 214.57058715820312, 'learning_rate': 5.9e-06, 'epoch': 0.07}
{'loss': 2.6354, 'grad_norm': 64.46117401123047, 'learning_rate': 6.900000000000001e-06, 'epoch': 0.08}
{'loss': 2.6281, 'grad_norm': 8.470099449157715, 'learning_rate': 7.9e-06, 'epoch': 0.09}
{'loss': 2.4856, 'grad_norm': 9.622514724731445, 'learning_rate': 8.9e-06, 'epoch': 0.1}
{'loss': 2.3529, 'grad_norm': 7.477068901062012, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.11}
{'loss': 2.334, 'grad_norm': 6.837642669677734, 'learning_rate': 1.09e-05, 'epoch': 0.12}
{'loss': 2.2118, 'grad_norm': 7.338916301727295, 'learning_rate': 1.19e-05, 'epoch': 0.13}
{'loss': 2.2106, 'grad_norm': 8.930304527282715, 'learning_rate': 1.29e-05, 'epoch': 0.14}
{'loss': 2.102, 'grad_norm': 7.030117511749268, 'learning_rate': 1.3900000000000002e-05, 'epoch': 0.15}
{'loss': 2.1413, 'grad_norm': 5.009380340576172, 'learning_rate': 1.49e-05, 'epoch': 0.16}
{'loss': 1.9946, 'grad_norm': 9.483946800231934, 'learning_rate': 1.59e-05, 'epoch': 0.17}
{'loss': 2.0052, 'grad_norm': 6.436705112457275, 'learning_rate': 1.69e-05, 'epoch': 0.18}
{'loss': 1.8776, 'grad_norm': 4.935334205627441, 'learning_rate': 1.79e-05, 'epoch': 0.2}
{'loss': 1.884, 'grad_norm': 4.823914051055908, 'learning_rate': 1.8900000000000002e-05, 'epoch': 0.21}
{'loss': 1.8841, 'grad_norm': 5.84005880355835, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.22}
{'loss': 1.9022, 'grad_norm': 5.328212738037109, 'learning_rate': 2.09e-05, 'epoch': 0.23}
{'loss': 1.8449, 'grad_norm': 4.852126121520996, 'learning_rate': 2.19e-05, 'epoch': 0.24}
{'loss': 1.8722, 'grad_norm': 5.616270542144775, 'learning_rate': 2.29e-05, 'epoch': 0.25}
{'loss': 1.8229, 'grad_norm': 4.568911552429199, 'learning_rate': 2.39e-05, 'epoch': 0.26}
{'loss': 1.789, 'grad_norm': 5.762459754943848, 'learning_rate': 2.4900000000000002e-05, 'epoch': 0.27}
{'loss': 1.7717, 'grad_norm': 4.0655341148376465, 'learning_rate': 2.5900000000000003e-05, 'epoch': 0.28}
{'loss': 1.7092, 'grad_norm': 4.430297374725342, 'learning_rate': 2.6900000000000003e-05, 'epoch': 0.29}
{'loss': 1.7882, 'grad_norm': 5.504685878753662, 'learning_rate': 2.7900000000000004e-05, 'epoch': 0.3}
{'loss': 1.8248, 'grad_norm': 5.337845802307129, 'learning_rate': 2.8899999999999998e-05, 'epoch': 0.31}
{'loss': 1.6906, 'grad_norm': 5.737122535705566, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.33}
{'loss': 1.8368, 'grad_norm': 5.350532531738281, 'learning_rate': 3.09e-05, 'epoch': 0.34}
{'loss': 1.9008, 'grad_norm': 3.924077033996582, 'learning_rate': 3.19e-05, 'epoch': 0.35}
{'loss': 1.8254, 'grad_norm': 5.931660175323486, 'learning_rate': 3.29e-05, 'epoch': 0.36}
{'loss': 1.7331, 'grad_norm': 3.8747799396514893, 'learning_rate': 3.3900000000000004e-05, 'epoch': 0.37}
{'loss': 1.7538, 'grad_norm': 24.208030700683594, 'learning_rate': 3.49e-05, 'epoch': 0.38}
{'loss': 1.6191, 'grad_norm': 5.137648582458496, 'learning_rate': 3.59e-05, 'epoch': 0.39}
{'loss': 1.707, 'grad_norm': 7.899143695831299, 'learning_rate': 3.69e-05, 'epoch': 0.4}
{'loss': 1.7079, 'grad_norm': 9.320395469665527, 'learning_rate': 3.79e-05, 'epoch': 0.41}
{'loss': 1.713, 'grad_norm': 5.005649089813232, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.42}
{'loss': 1.7037, 'grad_norm': 4.049073219299316, 'learning_rate': 3.99e-05, 'epoch': 0.43}
{'loss': 1.7084, 'grad_norm': 4.9181413650512695, 'learning_rate': 4.09e-05, 'epoch': 0.45}
{'loss': 1.6293, 'grad_norm': 4.726325511932373, 'learning_rate': 4.19e-05, 'epoch': 0.46}
{'loss': 1.7608, 'grad_norm': 4.952793598175049, 'learning_rate': 4.29e-05, 'epoch': 0.47}
{'loss': 1.6985, 'grad_norm': 4.534994125366211, 'learning_rate': 4.39e-05, 'epoch': 0.48}
{'loss': 1.6557, 'grad_norm': 4.268783092498779, 'learning_rate': 4.49e-05, 'epoch': 0.49}
{'loss': 1.7061, 'grad_norm': 4.482753753662109, 'learning_rate': 4.5900000000000004e-05, 'epoch': 0.5}
{'loss': 1.7049, 'grad_norm': 3.517976999282837, 'learning_rate': 4.69e-05, 'epoch': 0.51}
{'loss': 1.6143, 'grad_norm': 4.126358509063721, 'learning_rate': 4.79e-05, 'epoch': 0.52}
{'loss': 1.6448, 'grad_norm': 3.906385660171509, 'learning_rate': 4.89e-05, 'epoch': 0.53}
{'loss': 1.6868, 'grad_norm': 3.7254672050476074, 'learning_rate': 4.99e-05, 'epoch': 0.54}
{'eval_loss': 1.5529775619506836, 'eval_runtime': 448.648, 'eval_samples_per_second': 1.823, 'eval_steps_per_second': 0.23, 'epoch': 0.54}
{'loss': 1.6699, 'grad_norm': 3.7012038230895996, 'learning_rate': 4.89311163895487e-05, 'epoch': 0.55}
{'loss': 1.6266, 'grad_norm': 3.76411771774292, 'learning_rate': 4.7743467933491685e-05, 'epoch': 0.56}
{'loss': 1.6872, 'grad_norm': 3.571275472640991, 'learning_rate': 4.655581947743468e-05, 'epoch': 0.58}
{'loss': 1.5878, 'grad_norm': 3.658956289291382, 'learning_rate': 4.536817102137767e-05, 'epoch': 0.59}
{'loss': 1.6872, 'grad_norm': 4.740455627441406, 'learning_rate': 4.418052256532067e-05, 'epoch': 0.6}
{'loss': 1.6923, 'grad_norm': 3.6256442070007324, 'learning_rate': 4.299287410926366e-05, 'epoch': 0.61}
{'loss': 1.7134, 'grad_norm': 6.00480318069458, 'learning_rate': 4.1805225653206655e-05, 'epoch': 0.62}
{'loss': 1.6339, 'grad_norm': 3.486675500869751, 'learning_rate': 4.061757719714965e-05, 'epoch': 0.63}
{'loss': 1.5429, 'grad_norm': 5.6006317138671875, 'learning_rate': 3.9429928741092636e-05, 'epoch': 0.64}
{'loss': 1.6642, 'grad_norm': 3.0470900535583496, 'learning_rate': 3.824228028503563e-05, 'epoch': 0.65}
{'loss': 1.5821, 'grad_norm': 3.8348004817962646, 'learning_rate': 3.7054631828978624e-05, 'epoch': 0.66}
{'loss': 1.6402, 'grad_norm': 6.359446048736572, 'learning_rate': 3.586698337292162e-05, 'epoch': 0.67}
{'loss': 1.6472, 'grad_norm': 6.097104549407959, 'learning_rate': 3.467933491686461e-05, 'epoch': 0.68}
{'loss': 1.6495, 'grad_norm': 5.636963367462158, 'learning_rate': 3.3491686460807606e-05, 'epoch': 0.7}
{'loss': 1.5303, 'grad_norm': 3.8801746368408203, 'learning_rate': 3.23040380047506e-05, 'epoch': 0.71}
{'loss': 1.5675, 'grad_norm': 9.010661125183105, 'learning_rate': 3.111638954869359e-05, 'epoch': 0.72}
{'loss': 1.5948, 'grad_norm': 4.263934135437012, 'learning_rate': 2.992874109263658e-05, 'epoch': 0.73}
{'loss': 1.5924, 'grad_norm': 3.899005651473999, 'learning_rate': 2.8741092636579575e-05, 'epoch': 0.74}
{'loss': 1.5604, 'grad_norm': 7.3006062507629395, 'learning_rate': 2.7553444180522565e-05, 'epoch': 0.75}
{'loss': 1.6272, 'grad_norm': 4.360103130340576, 'learning_rate': 2.636579572446556e-05, 'epoch': 0.76}
{'loss': 1.5973, 'grad_norm': 4.11176872253418, 'learning_rate': 2.5178147268408553e-05, 'epoch': 0.77}
{'loss': 1.5865, 'grad_norm': 2.8876359462738037, 'learning_rate': 2.3990498812351544e-05, 'epoch': 0.78}
{'loss': 1.5446, 'grad_norm': 3.3186209201812744, 'learning_rate': 2.2802850356294538e-05, 'epoch': 0.79}
{'loss': 1.6598, 'grad_norm': 5.094956874847412, 'learning_rate': 2.161520190023753e-05, 'epoch': 0.8}
{'loss': 1.5687, 'grad_norm': 3.745133876800537, 'learning_rate': 2.0427553444180522e-05, 'epoch': 0.81}
{'loss': 1.6072, 'grad_norm': 4.093795299530029, 'learning_rate': 1.9239904988123516e-05, 'epoch': 0.83}
{'loss': 1.5681, 'grad_norm': 6.207923412322998, 'learning_rate': 1.8052256532066507e-05, 'epoch': 0.84}
{'loss': 1.5567, 'grad_norm': 4.961015701293945, 'learning_rate': 1.6864608076009504e-05, 'epoch': 0.85}
{'loss': 1.547, 'grad_norm': 4.069565773010254, 'learning_rate': 1.5676959619952495e-05, 'epoch': 0.86}
{'loss': 1.6665, 'grad_norm': 4.101574897766113, 'learning_rate': 1.4489311163895489e-05, 'epoch': 0.87}
{'loss': 1.5465, 'grad_norm': 3.346803903579712, 'learning_rate': 1.3301662707838481e-05, 'epoch': 0.88}
{'loss': 1.6205, 'grad_norm': 3.7059898376464844, 'learning_rate': 1.2114014251781473e-05, 'epoch': 0.89}
{'loss': 1.6289, 'grad_norm': 4.566877841949463, 'learning_rate': 1.0926365795724467e-05, 'epoch': 0.9}
{'loss': 1.5236, 'grad_norm': 6.136003017425537, 'learning_rate': 9.73871733966746e-06, 'epoch': 0.91}
{'loss': 1.546, 'grad_norm': 9.349658966064453, 'learning_rate': 8.551068883610452e-06, 'epoch': 0.92}
{'loss': 1.5574, 'grad_norm': 3.924513816833496, 'learning_rate': 7.363420427553444e-06, 'epoch': 0.93}
{'loss': 1.5614, 'grad_norm': 3.2035768032073975, 'learning_rate': 6.175771971496437e-06, 'epoch': 0.94}
{'loss': 1.5283, 'grad_norm': 3.142526388168335, 'learning_rate': 4.98812351543943e-06, 'epoch': 0.96}
{'loss': 1.5225, 'grad_norm': 3.188535213470459, 'learning_rate': 3.8004750593824232e-06, 'epoch': 0.97}
{'loss': 1.58, 'grad_norm': 4.337242603302002, 'learning_rate': 2.612826603325416e-06, 'epoch': 0.98}
{'loss': 1.5371, 'grad_norm': 3.9310617446899414, 'learning_rate': 1.4251781472684086e-06, 'epoch': 0.99}
{'loss': 1.5845, 'grad_norm': 4.091092586517334, 'learning_rate': 2.3752969121140145e-07, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████| 921/921 [7:38:49<00:00, 25.59s/it]C:\Users\preethisomayajula\anaconda3\envs\textS\lib\site-packages\transformers\modeling_utils.py:4034: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
{'train_runtime': 27568.6414, 'train_samples_per_second': 0.534, 'train_steps_per_second': 0.033, 'train_loss': 1.8268174022339065, 'epoch': 1.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 921/921 [7:39:28<00:00, 29.93s/it]
[2025-09-04 18:59:04,288: INFO: main: >>>>>> stage Model Training stage completed <<<<<<

x==========x]